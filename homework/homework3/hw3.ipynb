{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "import urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# Служебная функция, открывает файл и пускает его в суп\n",
    "def opener(link):\n",
    "    try:\n",
    "        a = urllib.request.urlopen(link)\n",
    "    except TimeoutError or urllib.error.HTTPError or urllib.error.URLError:\n",
    "        file = 'Скачивание не получилось'\n",
    "        file = BeautifulSoup(file, 'html.parser')\n",
    "    else:\n",
    "        file = a.read()\n",
    "        file = BeautifulSoup(file, 'html.parser')\n",
    "    return file\n",
    "\n",
    "\n",
    "# Достаю ссылки на номера газеты\n",
    "def getting_volumes(startlink):\n",
    "    volumes = []\n",
    "    i = 0\n",
    "    link = startlink + '/archive'\n",
    "    regvolume = re.compile('item\">\\s<a data-url=\"\" href=\"(.+?)\"')\n",
    "    while True:  # ради реюзабельности сделала, чтоб листал, пока листается\n",
    "        i += 1\n",
    "        page = link + '?page=' + str(i)\n",
    "        page = str(opener(page))\n",
    "        links = regvolume.findall(page)\n",
    "        if links:\n",
    "            for sublink in links:\n",
    "                volume = startlink + sublink\n",
    "                volumes.append(volume)\n",
    "        else:\n",
    "            break\n",
    "    return volumes\n",
    "\n",
    "\n",
    "# Достаю ссылки на отдельные статьи\n",
    "def getting_articles(volumes, startlink):\n",
    "    articles = []\n",
    "    regarticle = re.compile('caption-sm\">\\s<a href=\"(.+?)\"')\n",
    "    for volume in volumes:\n",
    "        volume = str(opener(volume))\n",
    "        a = regarticle.findall(volume)\n",
    "        # здесь возможно нужно такое же условие, как и выше\n",
    "        for sublink in a:\n",
    "            article = startlink + sublink\n",
    "            articles.append(article)\n",
    "    return articles\n",
    "\n",
    "\n",
    "# Я не придумала, как умнее конвертировать даты из слов в числа\n",
    "def months(string):\n",
    "    # есть еще вариант через .startswith(), но там проблема с мартом и маем\n",
    "    if string == 'января':\n",
    "        string = '01'\n",
    "    elif string == 'февраля':\n",
    "        string = '02'\n",
    "    elif string == 'марта':\n",
    "        string = '03'\n",
    "    elif string == 'апреля':\n",
    "        string = '04'\n",
    "    elif string == 'мая':\n",
    "        string = '05'\n",
    "    elif string == 'июня':\n",
    "        string = '06'\n",
    "    elif string == 'июля':\n",
    "        string = '07'\n",
    "    elif string == 'августа':\n",
    "        string = '08'\n",
    "    elif string == 'сентября':\n",
    "        string = '09'\n",
    "    elif string == 'октября':\n",
    "        string = '10'\n",
    "    elif string == 'ноября':\n",
    "        string = '11'\n",
    "    elif string == 'декабря':\n",
    "        string = '12'\n",
    "    return string\n",
    "\n",
    "\n",
    "# Этой функцией я собираю метаинформацию про каждую статью\n",
    "def meta(article):\n",
    "    # в моей газете нет категорий и авторов\n",
    "    # я ищу только текст статьи, название, дату создания\n",
    "    article = str(opener(article))\n",
    "    try:\n",
    "        header = re.search('<h2>(.+?)</h2>', article).group()\n",
    "        header = re.findall('<h2>(.+?)</h2>', header)[0]\n",
    "        created = re.findall('№ \\d+, (.+?)</a>', article)\n",
    "        for i, c in enumerate(created):\n",
    "            day = re.search('\\d+', c).group()\n",
    "            month = re.search('\\D+', c).group()\n",
    "            month = months(month.strip().strip(','))\n",
    "            year = re.search(', \\d+', c).group()\n",
    "            year = year.strip(',').strip()\n",
    "        date = day + '.' + month + '.' + year\n",
    "        # если я потом придумаю, как чистить, я залью ещё один файл\n",
    "        # с чистильщиком, автоматически гуляющим по директории\n",
    "        text = article\n",
    "    except AttributeError:\n",
    "        header, date, day, month, year, text = 'None', 'None', 'None', 'None', 'None', 'None'\n",
    "    return header, date, day, month, year, text\n",
    "\n",
    "\n",
    "# Эта функция создает или дополняет мета-файл\n",
    "def metafile(path, page, link, deleter):\n",
    "    header, date, day, month, year, text = page\n",
    "    meta_cort = (path, header, date, link, year)\n",
    "    meta_string = '%s\\tNone\\t%s\\t%s\\tпублицистика\\tNone\\tнейтральный\\tн-возраст\\t' \\\n",
    "                  'н-уровень\\tобластная\\t%s\\tКрасный Север\\t%s\\tгазета\\tРоссия\\t' \\\n",
    "                  'Вологодский регион\\tru\\n' % meta_cort\n",
    "    directory = './gazety'\n",
    "    meta_file = os.path.join(directory, 'metadata.csv')\n",
    "    if not deleter:\n",
    "        if not os.path.exists(meta_file):\n",
    "            meta_1 = 'path\\tauthor\\theader\\tcreated\\tsphere\\ttopic\\tstyle\\t' \\\n",
    "                     'audience_age\\taudience_level\\taudience_size\\tsource\\t' \\\n",
    "                     'publication\\tpubl_year\\tmedium\\tcountry\\tregion\\tlanguage\\n'\n",
    "            with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "                f.write(meta_1)\n",
    "            with open(meta_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(meta_string)\n",
    "        else:\n",
    "            a = False\n",
    "            with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "                if meta_string not in f:\n",
    "                    a = True\n",
    "            if a:\n",
    "                with open(meta_file, 'a', encoding='utf-8') as f:\n",
    "                    f.write(meta_string)\n",
    "    else:\n",
    "        # нашла решение для проблемы, когда почему-то теряется формат файла\n",
    "        with open(meta_file, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "        with open(meta_file, 'w', encoding='utf-8') as f:\n",
    "            f.writelines(item for item in lines[:-1])\n",
    "\n",
    "\n",
    "# У меня почему-то стрип плохо чистит, поэтому делаю функцию\n",
    "def adv_stripping(string):\n",
    "    string = string.replace('?', '\"')\n",
    "    string = string.replace('\"', '!')\n",
    "    string = string.replace('!', '.')\n",
    "    string = string.replace('/', '')\n",
    "    string = string.replace('.', '')\n",
    "    string = string.replace('*', '')\n",
    "    string = string.replace('<', '>')\n",
    "    string = string.replace(':', '')\n",
    "    string = string.replace('\\ ', '')\n",
    "    return string\n",
    "\n",
    "\n",
    "# Эта функция создает файловую структуру\n",
    "def fileconst(page, link):\n",
    "    header, date, day, month, year, text = page\n",
    "    for_article = ('None', header, date, 'None', link, text)\n",
    "    article = '@au %s @ti %s @da %s @topic %s @url %s\\n\\%s' % for_article\n",
    "    directory = './gazety'\n",
    "    dname = os.path.join(directory, 'plain', year, month, day)\n",
    "    if not os.path.exists(dname):\n",
    "        os.makedirs(dname)\n",
    "    header = adv_stripping(header)\n",
    "    path_1 = header + '.txt'\n",
    "    path = os.path.join(dname, path_1)\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            with open(path, 'w', encoding='utf-8') as f:\n",
    "                f.write(article)\n",
    "        except OSError:\n",
    "            print('Не получилось создать ',path_1)\n",
    "    deleter = False\n",
    "    metafile(path, page, link, deleter)\n",
    "\n",
    "    \n",
    "# А вот основная функция, запускающая краулер и создающая файловую систему\n",
    "def main():\n",
    "    print('Эта программа создаст у Вас на компьютере файловую систему.')\n",
    "    print('Убедитесь в том, что у Вас есть хотя бы 5 Гигабайт дискового пространства')\n",
    "    startlink = 'http://krassever.ru'\n",
    "    print('Качаю ссылки на номера газеты')\n",
    "    volumes = getting_volumes(startlink)\n",
    "    print('Скачал номера, качаю ссылки на статьи. Это занимает ~ 10 минут,'\n",
    "          'так что можете пока заварить себе чаю :)')\n",
    "    articles = getting_articles(volumes, startlink)\n",
    "    print('Всё скачал!')\n",
    "    a = input('Погнали? Если да, нажмите Enter. Если нет, введите любой символ: ')\n",
    "    while a == '':\n",
    "        n = 0\n",
    "        for link in articles:\n",
    "            page = meta(link)\n",
    "            fileconst(page, link)\n",
    "            n += 1\n",
    "            print(n)\n",
    "        print('Я скачал ', n, 'из', len(articles), 'страниц')\n",
    "        a = input('Могу пройтись ещё раз. Скачанные файлы не удалятся (да = Enter)')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
